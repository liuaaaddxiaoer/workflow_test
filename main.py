
import asyncio
import re
import os
from lxml import html
from redis.asyncio import Redis
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, MemoryAdaptiveDispatcher, RateLimiter, \
    GeolocationConfig
from playwright.async_api import Page

IS_DOCKER = True

REDIS_URL = "rediss://default:ASLRAAImcDFlZDM0YzQ3NTUxNTI0MjliYmVjNGFiYjBmZTExNzA2MXAxODkxMw@balanced-hagfish-8913.upstash.io:6379"
redis = Redis.from_url(REDIS_URL, decode_responses=True)

async def main():
    # --- 1. ç§å­åˆå§‹åŒ– (ä»…éœ€æ‰§è¡Œä¸€æ¬¡) ---
    # æ£€æŸ¥é˜Ÿåˆ—ï¼Œå¦‚æœæ²¡ä»»åŠ¡äº†ï¼Œç”±ç¬¬ä¸€ä¸ªå¯åŠ¨çš„å®¹å™¨è´Ÿè´£â€œè¡¥è´§â€
    if await redis.llen('task_queue_list') == 0:
        print("Initializing task queue...")
        initial_urls = [f'https://jable.tv/hot/{i}/' for i in range(1, 1469)]
        await redis.rpush('task_queue_list', *initial_urls)

    # --- 2. æµè§ˆå™¨ Hook ä¸æ‹¦æˆªé…ç½® (ä¿æŒä½ åŸæœ‰é€»è¾‘) ---
    async def _abort(route, request):
        if any(x in request.url for x in [".jpg", ".png", ".mp4", ".woff2"]):
            await route.abort()
        else:
            await route.continue_()

    async def _on_page_context_created(page: Page, **kwargs):
        await page.route('**/*', _abort)

    # Docker å»ºè®®ä½¿ç”¨ headless=True
    browser_cfg = BrowserConfig(
        headless=True if IS_DOCKER else False,
        enable_stealth=True,
        extra_args=["--disable-blink-features=AutomationControlled", "--no-sandbox","--start-maximized", "--disable-dev-shm-usage"]
    )

    run_cfg = CrawlerRunConfig(
        user_agent_mode='random',
        magic=True,
        stream=True,
        page_timeout=15000,
        locale='en-US',
        timezone_id='Asia/Singapore',
        mean_delay=1.2,
        max_range=0.5,
        geolocation=GeolocationConfig(latitude=1.364917, longitude=103.8198)
    )

    # ä¿æŒä½ åŸæœ‰çš„ Dispatcher é…ç½®
    rate1 = RateLimiter(base_delay=(2, 5))
    dipatcher = MemoryAdaptiveDispatcher(rate_limiter=rate1)
    dipatcher.max_session_permit = 4

    rate2 = RateLimiter(base_delay=(2, 4))
    dipatcher2 = MemoryAdaptiveDispatcher(rate_limiter=rate2)
    dipatcher2.max_session_permit = 8

    # --- 3. æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ ---
    async with AsyncWebCrawler(config=browser_cfg) as f:
        f.crawler_strategy.set_hook('on_page_context_created', _on_page_context_created)

        while True:
            # åˆ†å¸ƒå¼é¢†å–ä»»åŠ¡ï¼šæ¯ä¸ªå®¹å™¨æ‹¿èµ° 1 ä¸ªåˆ—è¡¨é¡µ
            current_list_url = await redis.lpop('task_queue_list')
            if not current_list_url:
                print("ğŸ æ‰€æœ‰ä»»åŠ¡å·²å¤„ç†å®Œæ¯•ï¼Œå®¹å™¨é€€å‡ºã€‚")
                break

            print(f"ğŸš€ æ­£åœ¨çˆ¬å–åˆ—è¡¨é¡µ: {current_list_url}")

            try:
                # çˆ¬å–åˆ—è¡¨é¡µ (ä½¿ç”¨åŸæœ‰ wait_for)
                res_gen = await f.arun_many(
                    [current_list_url],
                    config=run_cfg.clone(wait_for=".video-img-box.mb-e-20"),
                    dispatcher=dipatcher
                )

                async for result in res_gen:
                    if not result.html: continue

                    doc = html.fromstring(result.html)
                    detail_links = doc.xpath(
                        '//div[@class="video-img-box mb-e-20"]/div[@class="img-box cover-md"]/a/@href')

                    if not detail_links: continue

                    # --- é«˜æ•ˆå»é‡ï¼šåªä¿ç•™æ²¡çˆ¬è¿‡çš„ ---
                    todo_items = []
                    for link in detail_links:
                        if not await redis.hexists('finish_m3u8_urls', link):
                            todo_items.append(link)

                    if not todo_items: continue

                    # çˆ¬å–è¯¦æƒ…é¡µæå– m3u8
                    item_res_gen = await f.arun_many(
                        todo_items,
                        config=run_cfg.clone(wait_for=".count"),
                        dispatcher=dipatcher2
                    )

                    async for item_result in item_res_gen:
                        if not item_result.html: continue

                        M3U8_PATTERN = re.compile(r"hlsUrl\s*=\s*['\"](https?://[^'\"]+?\.m3u8[^'\"]*?)['\"]")
                        match = M3U8_PATTERN.search(item_result.html)

                        if match:
                            m3u8_url = match.group(1)
                            # å­˜å…¥ Redis Set (ç”¨äºå»é‡) å’Œ Hash (ç”¨äºè®°å½•è¯¦æƒ…)
                            # sismember åœ¨è¿™é‡Œå¯ä»¥åŒé‡ä¿é™©
                            if not await redis.sismember('m3u8_urls', m3u8_url):
                                await redis.sadd('m3u8_urls', m3u8_url)
                                await redis.hset('finish_m3u8_urls', item_result.url, m3u8_url)
                                print(f"âœ… æˆåŠŸ: {item_result.url} -> {m3u8_url}")
                        else:
                            # è®°å½•æœªæ‰¾åˆ°çš„ï¼Œé¿å…é‡å¤å°è¯•
                            await redis.sadd('no_m3u8_urls', item_result.url)

            except Exception as e:
                print(f"ğŸ”¥ è¿è¡Œå‡ºé”™ {current_list_url}: {e}")
                # å‡ºé”™äº†å¯ä»¥æŠŠä»»åŠ¡å¡å›é˜Ÿåˆ—æœ«å°¾é‡è¯•
                await redis.rpush('task_queue_list', current_list_url)


if __name__ == "__main__":
    asyncio.run(main())